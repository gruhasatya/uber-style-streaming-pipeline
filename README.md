# ðŸš– Ride-Hailing Real-Time Data Pipeline on AWS

## Why This Project?

Modern ride-hailing apps (like Uber or Lyft) rely on **real-time data pipelines** to:

* Track **driver GPS locations** continuously.
* Process **ride requests** as they happen.
* Detect **surge pricing conditions** when demand > supply.
* Match drivers with riders instantly.
* Store events for **analytics and reporting**.

This project simulates that architecture using **Apache Kafka (MSK), Apache Spark (EMR/Glue), AWS S3, Glue, Athena, and QuickSight**.

It demonstrates **both streaming (real-time) and batch (historical analytics)** on AWS, following industry standards.

---

##  Architecture Overview

![AWS Ride-Hailing Architecture](.rid-hailing-app/Ride-hailing.png)

**Flow**:

1. **Producers** â†’ send GPS & Ride request events to **Kafka (MSK)**.
2. **Spark Jobs (EMR / Glue Streaming)** â†’

   * Detect surge â†’ publish to Kafka.
   * Match drivers with rides â†’ publish to Kafka.
   * Archive events to **S3 Data Lake (Bronze â†’ Silver â†’ Gold)**.
3. **Consumers (EC2/Lambda Services)** â†’

   * Pricing, Notifications, Dashboard, Dispatch.
4. **Analytics** â†’ S3 â†’ Glue Crawler â†’ Athena â†’ QuickSight dashboards.
5. **Monitoring** â†’ CloudWatch for logs, metrics, alarms.

---

## Project Structure

```bash
ride-hailing-pipeline/
â”‚â”€â”€ README.md
â”‚â”€â”€ docker-compose.yml          # Local Kafka (optional testing)
â”‚â”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ gps_producer.py         # Simulates driver GPS events
â”‚   â””â”€â”€ rides_producer.py       # Simulates ride requests
â”‚
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ surge_detection.py      # Detects surge pricing, writes to Kafka
â”‚   â”œâ”€â”€ driver_matching.py      # Matches rides with drivers, writes to Kafka
â”‚   â””â”€â”€ batch_to_s3.py          # Archives raw data into S3 (Bronze)
â”‚
â”œâ”€â”€ consumers/
â”‚   â”œâ”€â”€ pricing_service_consumer.py
â”‚   â”œâ”€â”€ notifications_consumer.py
â”‚   â”œâ”€â”€ dashboard_consumer.py
â”‚   â””â”€â”€ dispatch_service_consumer.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logging_utils.py        # Centralized logging
â”‚   â””â”€â”€ metrics_utils.py        # Simple monitoring counters
â”‚
â””â”€â”€ images/
    â””â”€â”€ aws_architecture.png    # Architecture diagram
```

---

##  EC2 Setup (for Producers & Consumers)

Launch an **Amazon Linux 2 EC2 instance** (t2.medium recommended).

### 1. Install dependencies

```bash
sudo yum update -y
sudo yum install -y python3 git
python3 -m venv venv
source venv/bin/activate
pip install kafka-python pyspark boto3
```

### 2. Connect EC2 to MSK

* Ensure EC2 is in the same VPC as your **MSK cluster**.
* Add security group rules to allow traffic on port `9092`.

Update Kafka bootstrap servers in your code:

```python
producer = KafkaProducer(
    bootstrap_servers="b-1.msk-cluster-xyz.amazonaws.com:9092",
    value_serializer=lambda v: json.dumps(v).encode("utf-8")
)
```

---

## Running the Pipeline

### Step 1: Start Producers

```bash
python producers/gps_producer.py
python producers/rides_producer.py
```

### Step 2: Submit Spark Jobs

If using **EMR**:

```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
    spark_jobs/surge_detection.py

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
    spark_jobs/driver_matching.py

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
    spark_jobs/batch_to_s3.py
```

If using **AWS Glue Streaming Jobs**, upload `.py` files in Glue Console and configure MSK + S3 connection.

### Step 3: Start Consumers

```bash
python consumers/pricing_service_consumer.py
python consumers/notifications_consumer.py
python consumers/dashboard_consumer.py
python consumers/dispatch_service_consumer.py
```

---

##  Data Lake & Analytics

1. **S3 Storage** (Medallion)

   * Bronze: raw events (`gps_data/`, `rides_data/`)
   * Silver: cleaned (deduped, normalized)
   * Gold: aggregates (zone surge stats, driver utilization)

2. **Glue Crawler**

   * Run crawler on `s3://ride-hailing-data/`
   * Tables created in **Glue Data Catalog**

3. **Athena Queries**
   Example:

   ```sql
   SELECT zone, COUNT(*) AS total_rides
   FROM rides_data
   WHERE status = 'requested'
   GROUP BY zone;
   ```

4. **QuickSight Dashboards**

   * Connect QuickSight to Athena
   * Create dashboards for:

     * Surge trends
     * Driver distribution
     * Ride demand by zone

---

##  Monitoring

* **CloudWatch Logs** â†’ Spark jobs, EC2 consumers, producers.
* **CloudWatch Metrics** â†’ custom counters from `metrics_utils.py`.
* **CloudWatch Alarms** â†’ trigger if Spark job fails or Kafka lag > threshold.

---

## âœ… Key Learnings

* Built an **end-to-end streaming + batch pipeline**.
* Applied **Medallion Architecture** on AWS S3.
* Combined **real-time (surge, dispatch)** with **batch analytics (Athena, QuickSight)**.
* Used **logging and monitoring best practices**.


